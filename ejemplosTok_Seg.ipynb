{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python [default]",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "nbpresent": {
      "slides": {
        "1c3a8d0b-0b7c-4867-88cf-18c42179fede": {
          "id": "1c3a8d0b-0b7c-4867-88cf-18c42179fede",
          "layout": "treemap",
          "prev": null,
          "regions": {
            "b05523f8-8c96-48c0-9893-e4c1ed8a9d23": {
              "attrs": {
                "height": 1,
                "pad": 0.01,
                "treemap:weight": 1,
                "width": 1,
                "x": 0,
                "y": 0
              },
              "id": "b05523f8-8c96-48c0-9893-e4c1ed8a9d23"
            }
          }
        }
      },
      "themes": {
        "default": "0d06f8c5-fd8a-4657-956b-1abcac8d9e08",
        "theme": {
          "0d06f8c5-fd8a-4657-956b-1abcac8d9e08": {
            "id": "0d06f8c5-fd8a-4657-956b-1abcac8d9e08",
            "palette": {
              "19cc588f-0593-49c9-9f4b-e4d7cc113b1c": {
                "id": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
                "rgb": [
                  252,
                  252,
                  252
                ]
              },
              "31af15d2-7e15-44c5-ab5e-e04b16a89eff": {
                "id": "31af15d2-7e15-44c5-ab5e-e04b16a89eff",
                "rgb": [
                  68,
                  68,
                  68
                ]
              },
              "50f92c45-a630-455b-aec3-788680ec7410": {
                "id": "50f92c45-a630-455b-aec3-788680ec7410",
                "rgb": [
                  155,
                  177,
                  192
                ]
              },
              "c5cc3653-2ee1-402a-aba2-7caae1da4f6c": {
                "id": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
                "rgb": [
                  43,
                  126,
                  184
                ]
              },
              "efa7f048-9acb-414c-8b04-a26811511a21": {
                "id": "efa7f048-9acb-414c-8b04-a26811511a21",
                "rgb": [
                  25.118061674008803,
                  73.60176211453744,
                  107.4819383259912
                ]
              }
            },
            "rules": {
              "blockquote": {
                "color": "50f92c45-a630-455b-aec3-788680ec7410"
              },
              "code": {
                "font-family": "Anonymous Pro"
              },
              "h1": {
                "color": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
                "font-family": "Lato",
                "font-size": 8
              },
              "h2": {
                "color": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
                "font-family": "Lato",
                "font-size": 6
              },
              "h3": {
                "color": "50f92c45-a630-455b-aec3-788680ec7410",
                "font-family": "Lato",
                "font-size": 5.5
              },
              "h4": {
                "color": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
                "font-family": "Lato",
                "font-size": 5
              },
              "h5": {
                "font-family": "Lato"
              },
              "h6": {
                "font-family": "Lato"
              },
              "h7": {
                "font-family": "Lato"
              },
              "pre": {
                "font-family": "Anonymous Pro",
                "font-size": 4
              }
            },
            "text-base": {
              "font-family": "Merriweather",
              "font-size": 4
            }
          },
          "857cac3c-9845-463a-82c9-991cb9a5aca1": {
            "backgrounds": {
              "dc7afa04-bf90-40b1-82a5-726e3cff5267": {
                "background-color": "31af15d2-7e15-44c5-ab5e-e04b16a89eff",
                "id": "dc7afa04-bf90-40b1-82a5-726e3cff5267"
              }
            },
            "id": "857cac3c-9845-463a-82c9-991cb9a5aca1",
            "palette": {
              "19cc588f-0593-49c9-9f4b-e4d7cc113b1c": {
                "id": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
                "rgb": [
                  252,
                  252,
                  252
                ]
              },
              "31af15d2-7e15-44c5-ab5e-e04b16a89eff": {
                "id": "31af15d2-7e15-44c5-ab5e-e04b16a89eff",
                "rgb": [
                  68,
                  68,
                  68
                ]
              },
              "50f92c45-a630-455b-aec3-788680ec7410": {
                "id": "50f92c45-a630-455b-aec3-788680ec7410",
                "rgb": [
                  197,
                  226,
                  245
                ]
              },
              "c5cc3653-2ee1-402a-aba2-7caae1da4f6c": {
                "id": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
                "rgb": [
                  43,
                  126,
                  184
                ]
              },
              "efa7f048-9acb-414c-8b04-a26811511a21": {
                "id": "efa7f048-9acb-414c-8b04-a26811511a21",
                "rgb": [
                  25.118061674008803,
                  73.60176211453744,
                  107.4819383259912
                ]
              }
            },
            "rules": {
              "a": {
                "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c"
              },
              "blockquote": {
                "color": "50f92c45-a630-455b-aec3-788680ec7410",
                "font-size": 3
              },
              "code": {
                "font-family": "Anonymous Pro"
              },
              "h1": {
                "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
                "font-family": "Merriweather",
                "font-size": 8
              },
              "h2": {
                "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
                "font-family": "Merriweather",
                "font-size": 6
              },
              "h3": {
                "color": "50f92c45-a630-455b-aec3-788680ec7410",
                "font-family": "Lato",
                "font-size": 5.5
              },
              "h4": {
                "color": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
                "font-family": "Lato",
                "font-size": 5
              },
              "h5": {
                "font-family": "Lato"
              },
              "h6": {
                "font-family": "Lato"
              },
              "h7": {
                "font-family": "Lato"
              },
              "li": {
                "color": "50f92c45-a630-455b-aec3-788680ec7410",
                "font-size": 3.25
              },
              "pre": {
                "font-family": "Anonymous Pro",
                "font-size": 4
              }
            },
            "text-base": {
              "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
              "font-family": "Lato",
              "font-size": 4
            }
          }
        }
      }
    },
    "colab": {
      "name": "ejemplosTok_Seg.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ATalaveraDev/natural-language-engineering/blob/main/ejemplosTok_Seg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbpresent": {
          "id": "4c3ba838-f644-4890-a61f-0d6c8088fb87"
        },
        "id": "1tu1z2qZB2Zr"
      },
      "source": [
        "Ejemplos de tokenización y segmentación"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4S9JmL8ZYRUB",
        "outputId": "be537690-8048-46d9-bac5-411e40689d0d"
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "j=0\n",
        "for i in sent_tokenize(\"U.S.A. is a big country. More than 1.000.000 people live in it!\"):\n",
        "  print (\"Oración %s: %s\" %(j, i))\n",
        "  j=j+1\n",
        "  print(\"tokens: \", word_tokenize(i))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Oración 0: U.S.A. is a big country.\n",
            "tokens:  ['U.S.A.', 'is', 'a', 'big', 'country', '.']\n",
            "Oración 1: More than 1.000.000 people live in it!\n",
            "tokens:  ['More', 'than', '1.000.000', 'people', 'live', 'in', 'it', '!']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "MzNKVbjdB2Z0"
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "j=0\n",
        "for i in sent_tokenize(\"USA es un gran país. ¡Más de un millón de personas viven en él!\",language='spanish'):\n",
        "  print (\"Oración %s: %s\" %(j, i))\n",
        "  j=+1\n",
        "  print(\"tokens: \", word_tokenize(i,language='spanish'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYzTjP1UL0Op",
        "outputId": "84a1d7ed-3d8d-4ba4-8dd1-05a01d5b4ea8"
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "for i in sent_tokenize(\"El Dr. Pérez vendrá a las 18:30. ¡A ver si nos atiende rápido!\",language='spanish'):\n",
        "  print (\"Oración %s: %s\" %(j, i))\n",
        "  j=+1\n",
        "  print(\"tokens: \", word_tokenize(i,language='spanish'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Oración 2: El Dr. Pérez empieza la consulta a las 18:30.\n",
            "tokens:  ['El', 'Dr.', 'Pérez', 'empieza', 'la', 'consulta', 'a', 'las', '18:30', '.']\n",
            "Oración 1: ¡A ver si nos atiende rápido!\n",
            "tokens:  ['¡A', 'ver', 'si', 'nos', 'atiende', 'rápido', '!']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSMJ-uP0L-Y8",
        "outputId": "04ab8c89-8ace-494a-cbfc-4933e1708f01"
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import regexp_tokenize\n",
        "\n",
        "regexp_tokenize(\"!Hola! ¿Cómo estáis?\",\"([\\w]+|[¡!?¿])\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['!', 'Hola', '!', '¿', 'Cómo', 'estáis', '10', '30', '?']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1PnEAB3mz9g",
        "outputId": "c5aeecfa-7ae2-447a-eed2-eecf1d643dae"
      },
      "source": [
        "import nltk\n",
        "#nltk.download('punkt')\n",
        "#nltk.download('gutenberg')\n",
        "from nltk.corpus import gutenberg\n",
        "gutenberg.fileids() # lista con ids de los archivos que conforman el corpus (nombre)\n",
        "print(\"%25s %10s %10s %10s\" %(\"fileid\", \"lmWords\", \"lmSents\", \"lds\"))\n",
        "for fileid in gutenberg.fileids():\n",
        "  cars = gutenberg.raw(fileid) # raw() devuelve la lista de caractéres \n",
        "  words = gutenberg.words(fileid) # words() devuelve la lista de palabras\n",
        "  sents = gutenberg.sents(fileid) # sents() devuelve la lista de oraciones\n",
        "  vocab = set(w.lower() for w in words) # el vocabulario son las palabras diferentes \n",
        "  num_cars = len(cars) # len() devuelve el número de elementos de la lista o conjunto\n",
        "  num_words = len(words)\n",
        "  num_sents = len(sents)\n",
        "  num_vocab = len(vocab)\n",
        "  lmWords=round(num_cars/num_words) # número medio de caracteres por palabra\n",
        "  lmSents=round(num_words/num_sents) # número medio de palabras por frase\n",
        "  lds=round(num_words/num_vocab) # diversidad o riqueza léxica \n",
        "  print(\"%25s %10s %10s %10s\" %(fileid, lmWords, lmSents, lds))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                   fileid    lmWords    lmSents        lds\n",
            "          austen-emma.txt          5         25         26\n",
            "    austen-persuasion.txt          5         26         17\n",
            "         austen-sense.txt          5         28         22\n",
            "            bible-kjv.txt          4         34         79\n",
            "          blake-poems.txt          5         19          5\n",
            "       bryant-stories.txt          4         19         14\n",
            "  burgess-busterbrown.txt          4         18         12\n",
            "        carroll-alice.txt          4         20         13\n",
            "      chesterton-ball.txt          5         20         12\n",
            "     chesterton-brown.txt          5         23         11\n",
            "  chesterton-thursday.txt          5         18         11\n",
            "    edgeworth-parents.txt          4         21         25\n",
            "   melville-moby_dick.txt          5         26         15\n",
            "      milton-paradise.txt          5         52         11\n",
            "   shakespeare-caesar.txt          4         12          9\n",
            "   shakespeare-hamlet.txt          4         12          8\n",
            "  shakespeare-macbeth.txt          4         12          7\n",
            "       whitman-leaves.txt          5         36         12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgzyTSDcK6Xe"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "stopwords.fileids()\n",
        "#english_stops = set(stopwords.words('english'))\n",
        "#english_stops\n",
        "#stopwords.words('spanish')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUXWQ3Ch5zhx",
        "outputId": "5b485bfb-3dbe-4339-f12a-22030f8c5874"
      },
      "source": [
        "print(stopwords.words('spanish'))\n",
        "print(stopwords.words('english'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', 'del', 'se', 'las', 'por', 'un', 'para', 'con', 'no', 'una', 'su', 'al', 'lo', 'como', 'más', 'pero', 'sus', 'le', 'ya', 'o', 'este', 'sí', 'porque', 'esta', 'entre', 'cuando', 'muy', 'sin', 'sobre', 'también', 'me', 'hasta', 'hay', 'donde', 'quien', 'desde', 'todo', 'nos', 'durante', 'todos', 'uno', 'les', 'ni', 'contra', 'otros', 'ese', 'eso', 'ante', 'ellos', 'e', 'esto', 'mí', 'antes', 'algunos', 'qué', 'unos', 'yo', 'otro', 'otras', 'otra', 'él', 'tanto', 'esa', 'estos', 'mucho', 'quienes', 'nada', 'muchos', 'cual', 'poco', 'ella', 'estar', 'estas', 'algunas', 'algo', 'nosotros', 'mi', 'mis', 'tú', 'te', 'ti', 'tu', 'tus', 'ellas', 'nosotras', 'vosotros', 'vosotras', 'os', 'mío', 'mía', 'míos', 'mías', 'tuyo', 'tuya', 'tuyos', 'tuyas', 'suyo', 'suya', 'suyos', 'suyas', 'nuestro', 'nuestra', 'nuestros', 'nuestras', 'vuestro', 'vuestra', 'vuestros', 'vuestras', 'esos', 'esas', 'estoy', 'estás', 'está', 'estamos', 'estáis', 'están', 'esté', 'estés', 'estemos', 'estéis', 'estén', 'estaré', 'estarás', 'estará', 'estaremos', 'estaréis', 'estarán', 'estaría', 'estarías', 'estaríamos', 'estaríais', 'estarían', 'estaba', 'estabas', 'estábamos', 'estabais', 'estaban', 'estuve', 'estuviste', 'estuvo', 'estuvimos', 'estuvisteis', 'estuvieron', 'estuviera', 'estuvieras', 'estuviéramos', 'estuvierais', 'estuvieran', 'estuviese', 'estuvieses', 'estuviésemos', 'estuvieseis', 'estuviesen', 'estando', 'estado', 'estada', 'estados', 'estadas', 'estad', 'he', 'has', 'ha', 'hemos', 'habéis', 'han', 'haya', 'hayas', 'hayamos', 'hayáis', 'hayan', 'habré', 'habrás', 'habrá', 'habremos', 'habréis', 'habrán', 'habría', 'habrías', 'habríamos', 'habríais', 'habrían', 'había', 'habías', 'habíamos', 'habíais', 'habían', 'hube', 'hubiste', 'hubo', 'hubimos', 'hubisteis', 'hubieron', 'hubiera', 'hubieras', 'hubiéramos', 'hubierais', 'hubieran', 'hubiese', 'hubieses', 'hubiésemos', 'hubieseis', 'hubiesen', 'habiendo', 'habido', 'habida', 'habidos', 'habidas', 'soy', 'eres', 'es', 'somos', 'sois', 'son', 'sea', 'seas', 'seamos', 'seáis', 'sean', 'seré', 'serás', 'será', 'seremos', 'seréis', 'serán', 'sería', 'serías', 'seríamos', 'seríais', 'serían', 'era', 'eras', 'éramos', 'erais', 'eran', 'fui', 'fuiste', 'fue', 'fuimos', 'fuisteis', 'fueron', 'fuera', 'fueras', 'fuéramos', 'fuerais', 'fueran', 'fuese', 'fueses', 'fuésemos', 'fueseis', 'fuesen', 'sintiendo', 'sentido', 'sentida', 'sentidos', 'sentidas', 'siente', 'sentid', 'tengo', 'tienes', 'tiene', 'tenemos', 'tenéis', 'tienen', 'tenga', 'tengas', 'tengamos', 'tengáis', 'tengan', 'tendré', 'tendrás', 'tendrá', 'tendremos', 'tendréis', 'tendrán', 'tendría', 'tendrías', 'tendríamos', 'tendríais', 'tendrían', 'tenía', 'tenías', 'teníamos', 'teníais', 'tenían', 'tuve', 'tuviste', 'tuvo', 'tuvimos', 'tuvisteis', 'tuvieron', 'tuviera', 'tuvieras', 'tuviéramos', 'tuvierais', 'tuvieran', 'tuviese', 'tuvieses', 'tuviésemos', 'tuvieseis', 'tuviesen', 'teniendo', 'tenido', 'tenida', 'tenidos', 'tenidas', 'tened']\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHF5-ZlpvIB8"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "for linea in open('BowieHeroes.txt','r'):\n",
        "    linea2=''\n",
        "    for pal in word_tokenize(linea):\n",
        "      if pal.lower() not in stopwords.words('english'):\n",
        "        linea2=linea2+pal+' '\n",
        "    print(\"Original: \" + linea)\n",
        "    print(\"Simplificada: \" + linea2 +'\\n') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKArPCiC04J7",
        "outputId": "0ce6496d-f0b0-43cb-8e90-4c64796166a2"
      },
      "source": [
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "texto = open('quijote.txt','rt').read()\n",
        "for linea in sent_tokenize(texto,language='spanish'):\n",
        "    linea2=''\n",
        "    for pal in word_tokenize(linea):\n",
        "      if pal.lower() not in stopwords.words('spanish'):\n",
        "        linea2=linea2+pal+' '\n",
        "    print(\"Original: \" + linea)\n",
        "    print(\"Simplificada: \" + linea2 +'\\n') "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original: En un lugar de la Mancha, de cuyo nombre no quiero acordarme, no ha mucho tiempo que vivía un hidalgo de los de lanza en astillero, adarga antigua, rocín flaco y galgo corredor.\n",
            "Simplificada: lugar Mancha , cuyo nombre quiero acordarme , tiempo vivía hidalgo lanza astillero , adarga antigua , rocín flaco galgo corredor . \n",
            "\n",
            "Original: Una olla de algo más vaca que carnero, salpicón las más noches, duelos y quebrantos los sábados, lantejas los viernes, algún palomino de añadidura los domingos, consumían las tres cuartas partes de su hacienda.\n",
            "Simplificada: olla vaca carnero , salpicón noches , duelos quebrantos sábados , lantejas viernes , algún palomino añadidura domingos , consumían tres cuartas partes hacienda . \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZGtIEoXITP4",
        "outputId": "23f41c12-5b77-445e-e29f-ffa38bc5efa9"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "stemmer=PorterStemmer()\n",
        "for linea in open('BowieHeroes.txt','r'):\n",
        "    linea2=''\n",
        "    for pal in word_tokenize(linea):\n",
        "        print (pal +\" \" +stemmer.stem(pal))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I I\n",
            ", ,\n",
            "I I\n",
            "wish wish\n",
            "you you\n",
            "could could\n",
            "swim swim\n",
            "Like like\n",
            "the the\n",
            "dolphins dolphin\n",
            ", ,\n",
            "like like\n",
            "dolphins dolphin\n",
            "can can\n",
            "swim swim\n",
            "Though though\n",
            "nothing noth\n",
            ", ,\n",
            "nothing noth\n",
            "will will\n",
            "keep keep\n",
            "us us\n",
            "together togeth\n",
            "We We\n",
            "can can\n",
            "beat beat\n",
            "them them\n",
            ", ,\n",
            "for for\n",
            "ever ever\n",
            "and and\n",
            "ever ever\n",
            "Oh Oh\n",
            "we we\n",
            "can can\n",
            "be be\n",
            "heroes hero\n",
            ", ,\n",
            "just just\n",
            "for for\n",
            "one one\n",
            "day day\n",
            "I I\n",
            ", ,\n",
            "I I\n",
            "will will\n",
            "be be\n",
            "king king\n",
            "And and\n",
            "you you\n",
            ", ,\n",
            "you you\n",
            "will will\n",
            "be be\n",
            "queen queen\n",
            "Though though\n",
            "nothing noth\n",
            "will will\n",
            "drive drive\n",
            "them them\n",
            "away away\n",
            "We We\n",
            "can can\n",
            "be be\n",
            "heroes hero\n",
            ", ,\n",
            "just just\n",
            "for for\n",
            "one one\n",
            "day day\n",
            "We We\n",
            "can can\n",
            "be be\n",
            "us us\n",
            ", ,\n",
            "just just\n",
            "for for\n",
            "one one\n",
            "day day\n",
            "I I\n",
            ", ,\n",
            "I I\n",
            "can can\n",
            "remember rememb\n",
            "( (\n",
            "I I\n",
            "remember rememb\n",
            ") )\n",
            "Standing stand\n",
            ", ,\n",
            "by by\n",
            "the the\n",
            "wall wall\n",
            "( (\n",
            "by by\n",
            "the the\n",
            "wall wall\n",
            ") )\n",
            "And and\n",
            "the the\n",
            "guns gun\n",
            ", ,\n",
            "shot shot\n",
            "above abov\n",
            "our our\n",
            "heads head\n",
            "( (\n",
            "over over\n",
            "our our\n",
            "heads head\n",
            ") )\n",
            "And and\n",
            "we we\n",
            "kissed kiss\n",
            ", ,\n",
            "as as\n",
            "though though\n",
            "nothing noth\n",
            "could could\n",
            "fall fall\n",
            "( (\n",
            "nothing noth\n",
            "could could\n",
            "fall fall\n",
            ") )\n",
            "And and\n",
            "the the\n",
            "shame shame\n",
            ", ,\n",
            "was wa\n",
            "on on\n",
            "the the\n",
            "other other\n",
            "side side\n",
            "Oh Oh\n",
            ", ,\n",
            "we we\n",
            "can can\n",
            "beat beat\n",
            "them them\n",
            ", ,\n",
            "for for\n",
            "ever ever\n",
            "and and\n",
            "ever ever\n",
            "Then then\n",
            "we we\n",
            "could could\n",
            "be be\n",
            "heroes hero\n",
            ", ,\n",
            "just just\n",
            "for for\n",
            "one one\n",
            "day day\n",
            "We We\n",
            "can can\n",
            "be be\n",
            "heroes hero\n",
            "We We\n",
            "can can\n",
            "be be\n",
            "heroes hero\n",
            "We We\n",
            "can can\n",
            "be be\n",
            "heroes hero\n",
            "Just just\n",
            "for for\n",
            "one one\n",
            "day day\n",
            "We We\n",
            "can can\n",
            "be be\n",
            "heroes hero\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYDB7hTgNKdv"
      },
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "stemmer=SnowballStemmer('spanish')\n",
        "texto = open('quijote.txt','rt').read()\n",
        "for linea in sent_tokenize(texto,language='spanish'):\n",
        "    linea2=''\n",
        "    for pal in word_tokenize(linea):\n",
        "      print (pal +\" \" +stemmer.stem(pal)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrwlC5TJOz5J"
      },
      "source": [
        "\n",
        "#no maneja las irregularidades de idomas como el español\n",
        "#funciona mejor con lenguajes poco flexivos como el inglés\n",
        "from nltk.stem import SnowballStemmer\n",
        "stemmer=SnowballStemmer('spanish')\n",
        "\n",
        "print(stemmer.stem('querido'))\n",
        "print(stemmer.stem('querer'))\n",
        "print(stemmer.stem('quiero'))\n",
        "\n",
        "print(stemmer.stem('gatito'))\n",
        "print(stemmer.stem('gato'))\n",
        "\n",
        "print(stemmer.stem('comido'))\n",
        "print(stemmer.stem('comer'))\n",
        "print(stemmer.stem('comía'))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PynaSXI3Tu4e"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('wordnet')\n",
        "stemmer=WordNetLemmatizer()\n",
        "for linea in open('BowieHeroes.txt','r'):\n",
        "    linea2=''\n",
        "    for pal in word_tokenize(linea):\n",
        "        print (pal +\" \" +stemmer.lemmatize(pal))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}